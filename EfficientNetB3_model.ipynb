{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOQeXWdmSrfVKM3fFiTiFz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincent4u/Plant_disease-classification-Msc_thesis-/blob/main/EfficientNetB3_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Importing system libraries***"
      ],
      "metadata": {
        "id": "PE7WBNUhehCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import pathlib\n",
        "import itertools\n",
        "\n",
        "# import data handling tools\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# import Deep learning Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Ignore Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print ('modules loaded')"
      ],
      "metadata": {
        "id": "NwmMqNmpen4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Creating important functions***"
      ],
      "metadata": {
        "id": "Z9oKIK9Qezm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Functions to Create DataFrame from Dataset*"
      ],
      "metadata": {
        "id": "0fsPxb0re80b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate file paths and corresponding labels from a given directory\n",
        "def define_paths(data_dir):\n",
        "    \"\"\"\n",
        "    This function iterates through directories and subdirectories within the provided data directory,\n",
        "    collecting file paths and assigning them labels based on the parent directory they reside in.\n",
        "\n",
        "    Parameters:\n",
        "    - data_dir: The root directory where the data is stored.\n",
        "\n",
        "    Returns:\n",
        "    - A tuple containing two lists: filepaths (a list of absolute paths to all files) and labels (a list of labels corresponding to each file).\n",
        "    \"\"\"\n",
        "    filepaths = []  # List to store absolute paths of all files\n",
        "    labels = []     # List to store labels for each file\n",
        "\n",
        "    # Get a list of all folders/directories in the data directory\n",
        "    folds = os.listdir(data_dir)\n",
        "    for fold in folds:\n",
        "        foldpath = os.path.join(data_dir, fold)  # Construct the full path to the current folder\n",
        "        filelist = os.listdir(foldpath)           # Get a list of all files in the current folder\n",
        "        for file in filelist:\n",
        "            fpath = os.path.join(foldpath, file)  # Construct the full path to the current file\n",
        "            filepaths.append(fpath)               # Append the file path to the list\n",
        "            labels.append(fold)                   # Append the label (folder name) to the list\n",
        "\n",
        "    return filepaths, labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to concatenate file paths and labels into a single DataFrame\n",
        "def define_df(files, classes):\n",
        "    \"\"\"\n",
        "    This function creates two pandas Series objects from the provided lists of file paths and labels,\n",
        "    then concatenates these series side by side into a single DataFrame. This DataFrame will serve as the input\n",
        "    for further processing, such as splitting into training, validation, and testing sets.\n",
        "\n",
        "    Parameters:\n",
        "    - files: A list of file paths.\n",
        "    - classes: A list of labels corresponding to each file.\n",
        "\n",
        "    Returns:\n",
        "    - A DataFrame with two columns: 'filepaths' and 'labels', where 'filepaths' contains the paths to the files\n",
        "      and 'labels' contains the corresponding labels.\n",
        "    \"\"\"\n",
        "    Fseries = pd.Series(files, name='filepaths')  # Create a Series object for file paths\n",
        "    Lseries = pd.Series(classes, name='labels')   # Create a Series object for labels\n",
        "    return pd.concat([Fseries, Lseries], axis=1)  # Concatenate the two Series along the column axis\n",
        "\n",
        "\n",
        "# Function to split the combined DataFrame into training, validation, and testing sets\n",
        "def split_data(data_dir):\n",
        "    \"\"\"\n",
        "    This function performs the following steps:\n",
        "    1. Generates file paths and labels using the `define_paths` function.\n",
        "    2. Combines these into a DataFrame using the `define_df` function.\n",
        "    3. Splits the resulting DataFrame into training, validation, and testing sets, ensuring that the distribution\n",
        "       of labels across these sets remains consistent (stratified sampling).\n",
        "\n",
        "    Parameters:\n",
        "    - data_dir: The root directory where the data is stored.\n",
        "\n",
        "    Returns:\n",
        "    - A tuple containing three DataFrames: train_df (for training), valid_df (for validation), and test_df (for testing).\n",
        "    \"\"\"\n",
        "    # Step 1: Generate file paths and labels\n",
        "    files, classes = define_paths(data_dir)\n",
        "    df = define_df(files, classes)\n",
        "\n",
        "    # Extracting the labels for stratification\n",
        "    strat = df['labels']\n",
        "\n",
        "    # Step 2: Split the DataFrame into training and dummy sets (80% training)\n",
        "    train_df, dummy_df = train_test_split(df, train_size=0.8, shuffle=True, random_state=123, stratify=strat)\n",
        "\n",
        "    # Step 3: Further split the dummy set into validation and testing sets (50% validation)\n",
        "    strat = dummy_df['labels']  # Re-extracting the labels for stratification\n",
        "    valid_df, test_df = train_test_split(dummy_df, train_size=0.5, shuffle=True, random_state=123, stratify=strat)\n",
        "\n",
        "    return train_df, valid_df, test_df\n",
        "\n"
      ],
      "metadata": {
        "id": "t0QeDnYEe5gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Function to generate trainig,validation and test split set from th diretory***"
      ],
      "metadata": {
        "id": "G6BGycQ6i5Hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gens(train_df, valid_df, test_df, batch_size):\n",
        "    '''\n",
        "    This function prepares image data generators for training, validation, and testing datasets.\n",
        "    It uses Keras' ImageDataGenerator to preprocess images and convert them into tensors\n",
        "    suitable for model training. The preprocessing includes resizing images to a standard size,\n",
        "    applying data augmentation techniques like horizontal flipping, and converting images\n",
        "    into categorical labels based on their file paths and labels columns.\n",
        "\n",
        "    Parameters:\n",
        "    - train_df: DataFrame containing training dataset with file paths and corresponding labels.\n",
        "    - valid_df: DataFrame containing validation dataset with file paths and corresponding labels.\n",
        "    - test_df: DataFrame containing testing dataset with file paths and corresponding labels.\n",
        "    - batch_size: Number of samples per gradient update during training time.\n",
        "\n",
        "    Returns:\n",
        "    - train_gen: An instance of ImageDataGenerator for the training set.\n",
        "    - valid_gen: An instance of ImageDataGenerator for the validation set.\n",
        "    - test_gen: An instance of ImageDataGenerator for the testing set, using a custom batch size calculated to ensure that the total number of batches does not exceed 80.\n",
        "    '''\n",
        "\n",
        "    # Define the dimensions of the input images and the number of color channels.\n",
        "    img_size = (224, 224)                                                       # Standard input size for the model.\n",
        "    channels = 3                                                                # Using RGB images.\n",
        "    color = 'rgb'                                                               # Color mode for the images.\n",
        "    img_shape = (img_size[0], img_size[1], channels)                            # Shape of the input images.\n",
        "\n",
        "    # Calculate the optimal batch size for the test set to ensure efficient memory usage.\n",
        "    ts_length = len(test_df)  # Total number of samples in the test set.\n",
        "    test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length % n == 0 and ts_length / n <= 80]))  # Find the largest divisor of ts_length that divides evenly and keeps the number of batches below 80.\n",
        "    test_steps = ts_length // test_batch_size                                   # Calculate the number of steps required to iterate over the entire test set.\n",
        "\n",
        "    # Define a simple preprocessing function that returns the input image unchanged.\n",
        "    def scalar(img):                                                                  # Placeholder function for demonstration purposes.\n",
        "        return img\n",
        "\n",
        "    # Create instances of ImageDataGenerator for both training and testing sets.\n",
        "    tr_gen = ImageDataGenerator(preprocessing_function=scalar, horizontal_flip=True)  # Apply horizontal flip for data augmentation.\n",
        "    ts_gen = ImageDataGenerator(preprocessing_function=scalar)                        # No data augmentation for the test set.\n",
        "\n",
        "    # Generate data generators for the training, validation, and test sets.\n",
        "    train_gen = tr_gen.flow_from_dataframe(\n",
        "        train_df,\n",
        "        x_col='filepaths',                                                             # Column name for file paths in the DataFrame.\n",
        "        y_col='labels',                                                                # Column name for labels in the DataFrame.\n",
        "        target_size=img_size,                                                          # Resize images to the specified size.\n",
        "        class_mode='categorical',                                                      # Use categorical labels.\n",
        "        color_mode=color,                                                              # Specify the color mode.\n",
        "        shuffle=True,                                                                  # Shuffle the data.\n",
        "        batch_size=batch_size                                                          # Batch size for training.\n",
        "    )\n",
        "\n",
        "    valid_gen = ts_gen.flow_from_dataframe(\n",
        "        valid_df,\n",
        "        x_col='filepaths',\n",
        "        y_col='labels',\n",
        "        target_size=img_size,\n",
        "        class_mode='categorical',\n",
        "        color_mode=color,\n",
        "        shuffle=True,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # For the test set, use the custom batch size calculation and disable shuffling.\n",
        "    test_gen = ts_gen.flow_from_dataframe(\n",
        "        test_df,\n",
        "        x_col='filepaths',\n",
        "        y_col='labels',\n",
        "        target_size=img_size,\n",
        "        class_mode='categorical',\n",
        "        color_mode=color,\n",
        "        shuffle=False,                                  # Do not shuffle the test set.\n",
        "        batch_size=test_batch_size\n",
        "    )\n",
        "\n",
        "    return train_gen, valid_gen, test_gen\n"
      ],
      "metadata": {
        "id": "wQsLKrz0jA-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing the images with thier respective labels**"
      ],
      "metadata": {
        "id": "ub-AEZv9konM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(gen):\n",
        "    '''\n",
        "    This function displays a sample of images generated by the given data generator.\n",
        "    It retrieves a batch of images along with their labels from the generator,\n",
        "    selects a subset of these images for display, and then plots them using matplotlib.\n",
        "    Each image is labeled with its predicted class name.\n",
        "\n",
        "    Parameters:\n",
        "    - gen: A data generator object, typically returned by methods like flow_from_directory() or\n",
        "    flow_from_dataframe() in Keras' ImageDataGenerator.\n",
        "\n",
        "    Notes:\n",
        "    - The function assumes that the data generator yields batches of images and their corresponding labels.\n",
        "    - The number of images displayed is limited to a maximum of 25 to avoid cluttering the plot.\n",
        "    - Images are normalized to the range [0, 255] for better visualization.\n",
        "    '''\n",
        "\n",
        "    # Retrieve the mapping between class indices and class names from the generator.\n",
        "    g_dict = gen.class_indices                                                        # Dictionary mapping class names to indices. defines dictionary {'class': index}\n",
        "    classes = list(g_dict.keys())                                                     # List of class names extracted from the dictionary keys.\n",
        "\n",
        "    # Fetch a batch of images and their labels from the generator.\n",
        "    images, labels = next(gen)                                                        # Get a batch of images and their labels.\n",
        "\n",
        "    # Determine the number of images to display, up to a maximum of 25.\n",
        "    length = len(labels)                                                              # Length of the batch.\n",
        "    sample = min(length, 25)                                                          # Limit the number of displayed images to 25 or fewer.\n",
        "\n",
        "    # Initialize a figure for plotting the images.\n",
        "    plt.figure(figsize=(20, 20))                                                      # Set the figure size.\n",
        "\n",
        "    # Loop through the selected images and plot each one.\n",
        "    for i in range(sample):\n",
        "        plt.subplot(5, 5, i + 1)                                                      # Position the subplot.\n",
        "\n",
        "        # Normalize the image pixel values to the range [0, 255].\n",
        "        image = images[i] / 255\n",
        "\n",
        "        # Display the image.\n",
        "        plt.imshow(image)\n",
        "\n",
        "        # Retrieve the label index of the current image.\n",
        "        index = np.argmax(labels[i])\n",
        "\n",
        "        # Extract the class name corresponding to the label index.\n",
        "        class_name = classes[index]\n",
        "\n",
        "        # Add a title to the subplot indicating the class name.\n",
        "        plt.title(class_name, color='blue', fontsize=12)\n",
        "\n",
        "        # Hide axes for cleaner visualization.\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Display the figure with the plotted images.\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "VQJt5EpXjMxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Implementing a callback***\n",
        "\n",
        "  \n",
        "    Custom callback class for dynamic learning rate adjustment and\n",
        "    early stopping based on training and validation metrics.\n",
        "\n",
        "    This callback dynamically adjusts the learning rate based on the performance of the model\n",
        "    on the training and validation datasets. It also implements early stopping based on a specified number\n",
        "    of epochs without improvement in validation loss.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The Keras model to which this callback is attached.\n",
        "    - patience: Number of epochs with no improvement after which learning rate is reduced.\n",
        "    - stop_patience: Number of times the learning rate can be reduced before stopping training.\n",
        "    - threshold: Training accuracy threshold for adjusting the learning rate based on validation loss.\n",
        "    - factor: Factor by which to reduce the learning rate.\n",
        "    - batches: Number of training batches to run per epoch.\n",
        "    - epochs: Total number of epochs to train for.\n",
        "    - ask_epoch: Optional parameter to prompt the user to decide whether to continue training past\n",
        "    a certain epoch.\n",
        "  "
      ],
      "metadata": {
        "id": "H_Ump9NMk7vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, model, patience, stop_patience, threshold, factor, batches, epochs, ask_epoch):\n",
        "        super(MyCallback, self).__init__()\n",
        "        self.model = model\n",
        "        self.patience = patience # specifies how many epochs without improvement before learning rate is adjusted\n",
        "        self.stop_patience = stop_patience # specifies how many times to adjust lr without improvement to stop training\n",
        "        self.threshold = threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n",
        "        self.factor = factor # factor by which to reduce the learning rate\n",
        "        self.batches = batches # number of training batch to run per epoch\n",
        "        self.epochs = epochs\n",
        "        self.ask_epoch = ask_epoch\n",
        "        self.ask_epoch_initial = ask_epoch # save this value to restore if restarting training\n",
        "\n",
        "        # callback variables\n",
        "        self.count = 0 # how many times lr has been reduced without improvement\n",
        "        self.stop_count = 0\n",
        "        self.best_epoch = 1   # epoch with the lowest loss\n",
        "        self.initial_lr = float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initial learning rate and save it\n",
        "        self.highest_tracc = 0.0 # set highest training accuracy to 0 initially\n",
        "        self.lowest_vloss = np.inf # set lowest validation loss to infinity initially\n",
        "        self.best_weights = self.model.get_weights() # set best weights to model's initial weights\n",
        "        self.initial_weights = self.model.get_weights()   # save initial weights if they have to get restored\n",
        "\n",
        "    # Define a function that will run when train begins\n",
        "    def on_train_begin(self, logs= None):\n",
        "        msg = 'Do you want model asks you to halt the training [y/n] ?'\n",
        "        print(msg)\n",
        "        ans = input('')\n",
        "        if ans in ['Y', 'y']:\n",
        "            self.ask_permission = 1\n",
        "        elif ans in ['N', 'n']:\n",
        "            self.ask_permission = 0\n",
        "\n",
        "        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
        "        print(msg)\n",
        "        self.start_time = time.time()\n",
        "\n",
        "\n",
        "    def on_train_end(self, logs= None):\n",
        "        stop_time = time.time()\n",
        "        tr_duration = stop_time - self.start_time\n",
        "        hours = tr_duration // 3600\n",
        "        minutes = (tr_duration - (hours * 3600)) // 60\n",
        "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
        "\n",
        "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
        "        print(msg)\n",
        "\n",
        "        # set the weights of the model to the best weights\n",
        "        self.model.set_weights(self.best_weights)\n",
        "\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs= None):\n",
        "        # get batch accuracy and loss\n",
        "        acc = logs.get('accuracy') * 100\n",
        "        loss = logs.get('loss')\n",
        "\n",
        "        # prints over on the same line to show running batch count\n",
        "        msg = '{0:20s}processing batch {1:} of {2:5s}-   accuracy=  {3:5.3f}   -   loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
        "        print(msg, '\\r', end= '')\n",
        "\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs= None):\n",
        "        self.ep_start = time.time()\n",
        "\n",
        "\n",
        "    # Define method runs on the end of each epoch\n",
        "    def on_epoch_end(self, epoch, logs= None):\n",
        "        ep_end = time.time()\n",
        "        duration = ep_end - self.ep_start\n",
        "\n",
        "        lr = float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
        "        current_lr = lr\n",
        "        acc = logs.get('accuracy')  # get training accuracy\n",
        "        v_acc = logs.get('val_accuracy')  # get validation accuracy\n",
        "        loss = logs.get('loss')  # get training loss for this epoch\n",
        "        v_loss = logs.get('val_loss')  # get the validation loss for this epoch\n",
        "\n",
        "        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n",
        "            monitor = 'accuracy'\n",
        "            if epoch == 0:\n",
        "                pimprov = 0.0\n",
        "            else:\n",
        "                pimprov = (acc - self.highest_tracc ) * 100 / self.highest_tracc # define improvement of model progres\n",
        "\n",
        "            if acc > self.highest_tracc: # training accuracy improved in the epoch\n",
        "                self.highest_tracc = acc # set new highest training accuracy\n",
        "                self.best_weights = self.model.get_weights() # training accuracy improved so save the weights\n",
        "                self.count = 0 # set count to 0 since training accuracy improved\n",
        "                self.stop_count = 0 # set stop counter to 0\n",
        "                if v_loss < self.lowest_vloss:\n",
        "                    self.lowest_vloss = v_loss\n",
        "                self.best_epoch = epoch + 1  # set the value of best epoch for this epoch\n",
        "\n",
        "            else:\n",
        "                # training accuracy did not improve check if this has happened for patience number of epochs\n",
        "                # if so adjust learning rate\n",
        "                if self.count >= self.patience - 1: # lr should be adjusted\n",
        "                    lr = lr * self.factor # adjust the learning by factor\n",
        "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
        "                    self.count = 0 # reset the count to 0\n",
        "                    self.stop_count = self.stop_count + 1 # count the number of consecutive lr adjustments\n",
        "                    self.count = 0 # reset counter\n",
        "                    if v_loss < self.lowest_vloss:\n",
        "                        self.lowest_vloss = v_loss\n",
        "                else:\n",
        "                    self.count = self.count + 1 # increment patience counter\n",
        "\n",
        "        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n",
        "            monitor = 'val_loss'\n",
        "            if epoch == 0:\n",
        "                pimprov = 0.0\n",
        "\n",
        "            else:\n",
        "                pimprov = (self.lowest_vloss - v_loss ) * 100 / self.lowest_vloss\n",
        "\n",
        "            if v_loss < self.lowest_vloss: # check if the validation loss improved\n",
        "                self.lowest_vloss = v_loss # replace lowest validation loss with new validation loss\n",
        "                self.best_weights = self.model.get_weights() # validation loss improved so save the weights\n",
        "                self.count = 0 # reset count since validation loss improved\n",
        "                self.stop_count = 0\n",
        "                self.best_epoch = epoch + 1 # set the value of the best epoch to this epoch\n",
        "\n",
        "            else: # validation loss did not improve\n",
        "                if self.count >= self.patience - 1: # need to adjust lr\n",
        "                    lr = lr * self.factor # adjust the learning rate\n",
        "                    self.stop_count = self.stop_count + 1 # increment stop counter because lr was adjusted\n",
        "                    self.count = 0 # reset counter\n",
        "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
        "\n",
        "                else:\n",
        "                    self.count = self.count + 1 # increment the patience counter\n",
        "\n",
        "                if acc > self.highest_tracc:\n",
        "                    self.highest_tracc = acc\n",
        "\n",
        "        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc * 100:^9.3f}{v_loss:^9.5f}{v_acc * 100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
        "        print(msg)\n",
        "\n",
        "        if self.stop_count > self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n",
        "            msg = f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
        "            print(msg)\n",
        "            self.model.stop_training = True # stop training\n",
        "\n",
        "        else:\n",
        "            if self.ask_epoch != None and self.ask_permission != 0:\n",
        "                if epoch + 1 >= self.ask_epoch:\n",
        "                    msg = 'enter H to halt training or an integer for number of epochs to run then ask again'\n",
        "                    print(msg)\n",
        "\n",
        "                    ans = input('')\n",
        "                    if ans == 'H' or ans == 'h':\n",
        "                        msg = f'training has been halted at epoch {epoch + 1} due to user input'\n",
        "                        print(msg)\n",
        "                        self.model.stop_training = True # stop training\n",
        "\n",
        "                    else:\n",
        "                        try:\n",
        "                            ans = int(ans)\n",
        "                            self.ask_epoch += ans\n",
        "                            msg = f' training will continue until epoch {str(self.ask_epoch)}'\n",
        "                            print(msg)\n",
        "                            msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor', '% Improv', 'Duration')\n",
        "                            print(msg)\n",
        "\n",
        "                        except Exception:\n",
        "                            print('Invalid')\n"
      ],
      "metadata": {
        "id": "Epf7Zcc3lQ7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Plot Function for visualizing the Training History indicating accuracy and loss results***"
      ],
      "metadata": {
        "id": "Kd6qAHUxlgez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training(hist):\n",
        "    '''\n",
        "    This function takes a training model's history object and plots the training and validation accuracy and loss over epochs,\n",
        "    highlighting the epoch with the best validation loss and accuracy.\n",
        "    '''\n",
        "\n",
        "    # Extracting training and validation metrics from the history object\n",
        "    tr_acc = hist.history['accuracy']                                       # Training accuracy\n",
        "    tr_loss = hist.history['loss']                                          # Training loss\n",
        "    val_acc = hist.history['val_accuracy']                                  # Validation accuracy\n",
        "    val_loss = hist.history['val_loss']                                     # Validation loss\n",
        "\n",
        "    # Identifying the epoch with the lowest validation loss and highest validation accuracy\n",
        "    index_loss = np.argmin(val_loss)                                        # Index of the epoch with the lowest validation loss\n",
        "    val_lowest = val_loss[index_loss]                                       # Value of the lowest validation loss\n",
        "    index_acc = np.argmax(val_acc)                                          # Index of the epoch with the highest validation accuracy\n",
        "    acc_highest = val_acc[index_acc]                                        # Value of the highest validation accuracy\n",
        "\n",
        "    # Creating a list of epochs for plotting\n",
        "    Epochs = [i+1 for i in range(len(tr_acc))]                              # Adjusting epoch numbers to start from 1\n",
        "\n",
        "    # Labels for the best epoch in terms of loss and accuracy\n",
        "    loss_label = f'best epoch= {str(index_loss + 1)}'                        # Label for the best epoch in terms of validation loss\n",
        "    acc_label = f'best epoch= {str(index_acc + 1)}'                          # Label for the best epoch in terms of validation accuracy\n",
        "\n",
        "    # Plotting training and validation loss\n",
        "    plt.figure(figsize=(20, 8))                                             # Setting figure size\n",
        "    plt.style.use('fivethirtyeight')                                        # Using a style for the plot\n",
        "\n",
        "    plt.subplot(1, 2, 1)                                                    # Subplot for loss\n",
        "    plt.plot(Epochs, tr_loss, 'r', label='Training loss')                   # Plotting training loss\n",
        "    plt.plot(Epochs, val_loss, 'g', label='Validation loss')                 # Plotting validation loss\n",
        "    plt.scatter(index_loss + 1, val_lowest, s=150, c='blue', label=loss_label)  # Highlighting the best epoch in terms of validation loss\n",
        "    plt.title('Training and Validation Loss')                                   # Title for the subplot\n",
        "    plt.xlabel('Epochs')                                                    # X-axis label\n",
        "    plt.ylabel('Loss')                                                      # Y-axis label\n",
        "    plt.legend()                                                            # Displaying legend\n",
        "\n",
        "    plt.subplot(1, 2, 2)                                                    # Subplot for accuracy\n",
        "    plt.plot(Epochs, tr_acc, 'r', label='Training Accuracy')                # Plotting training accuracy\n",
        "    plt.plot(Epochs, val_acc, 'g', label='Validation Accuracy')             # Plotting validation accuracy\n",
        "    plt.scatter(index_acc + 1, acc_highest, s=150, c='blue', label=acc_label)  # Highlighting the best epoch in terms of validation accuracy\n",
        "    plt.title('Training and Validation Accuracy')                               # Title for the subplot\n",
        "    plt.xlabel('Epochs')                                                        # X-axis label\n",
        "    plt.ylabel('Accuracy')                                                      # Y-axis label\n",
        "    plt.legend()                                                                # Displaying legend\n",
        "\n",
        "    plt.tight_layout()                                                          # Adjusting layout to prevent overlapping labels\n",
        "    plt.show()                                                                  # Displaying the plot\n"
      ],
      "metadata": {
        "id": "AHYSPzPLl0eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Loading the Dataset from Kaggle Database***\n",
        "\n",
        "This also involves utilizing the already exisitng functions like\n",
        "def define_paths(data_dir)\n",
        "def define_df(files, classes)\n",
        "def split_data(data_dir)\n",
        "def create_gens(train_df, valid_df, test_df, batch_size)\n",
        "def show_images(gen)\n"
      ],
      "metadata": {
        "id": "ap6BgSicmScz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I have already done step 1 with the seceret thing\n",
        "\n",
        "#Step 2: Accessing and Exporting Kaggle secrets to the environment\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "\n",
        "#Step 3: Download Dataset :\n",
        "!kaggle datasets download -d emmarex/plantdisease\n",
        "# !kaggle datasets download -d abdallahalidev/plantvillage-dataset\n",
        "\n",
        "! mkdir data_sets\n",
        "! unzip plantdisease.zip -d data_sets\n",
        "\n",
        "# ! mkdir data_vin\n",
        "# ! unzip plantvillage-dataset.zip -d data_vin\n",
        "\n",
        "\n",
        "data_dir = '/content/data_sets/plantvillage/PlantVillage'\n",
        "try:\n",
        "    # Get splitted data\n",
        "    train_df, valid_df, test_df = split_data(data_dir)\n",
        "\n",
        "    # Get Generators\n",
        "    batch_size = 40\n",
        "    train_gen, valid_gen, test_gen = create_gens(train_df, valid_df, test_df, batch_size)\n",
        "    show_images(train_gen)\n",
        "except:\n",
        "    print('Invalid Input')\n"
      ],
      "metadata": {
        "id": "UAY0axK9mcE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Structuring the Models achitecture and training the Model utilizing the callback function***"
      ],
      "metadata": {
        "id": "urwjCWvemyQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create Model Structure\n",
        "img_size = (224, 224)\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n",
        "\n",
        "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
        "# we will use efficientnetb3 from EfficientNet family.\n",
        "base_model = tf.keras.applications.efficientnet.EfficientNetB3(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n",
        "    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n",
        "                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n",
        "    Dropout(rate= 0.45, seed= 123),\n",
        "    Dense(class_count, activation= 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "batch_size = 40   # set batch size for training\n",
        "epochs = 40   # number of all epochs in training\n",
        "patience = 1   #number of epochs to wait to adjust lr if monitored value does not improve\n",
        "stop_patience = 3   # number of epochs to wait before stopping training if monitored value does not improve\n",
        "threshold = 0.9   # if train accuracy is < threshold adjust monitor accuracy, else monitor validation loss\n",
        "factor = 0.5   # factor to reduce lr by\n",
        "ask_epoch = 5   # number of epochs to run before asking if you want to halt training\n",
        "batches = int(np.ceil(len(train_gen.labels) / batch_size))    # number of training batch to run per epoch\n",
        "\n",
        "callbacks = [MyCallback(model= model, patience= patience, stop_patience= stop_patience, threshold= threshold,\n",
        "            factor= factor, batches= batches, epochs= epochs, ask_epoch= ask_epoch )]\n",
        "\n",
        "history = model.fit(x= train_gen, epochs= epochs, verbose= 0, callbacks= callbacks,\n",
        "                    validation_data= valid_gen, validation_steps= None, shuffle= False)"
      ],
      "metadata": {
        "id": "vvHhGW_koDeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Evaluating the Models performance ***"
      ],
      "metadata": {
        "id": "K0GnuPcYoYdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plot_training(history)\n",
        "\n",
        "# Determine the test batch size that evenly divides the test dataset and is less than or equal to 80\n",
        "ts_length = len(test_df)\n",
        "test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length % n == 0 and ts_length / n <= 80]))\n",
        "test_steps = ts_length // test_batch_size\n",
        "\n",
        "# Evaluate the model on the training, validation, and test sets\n",
        "train_score = model.evaluate(train_gen, steps=test_steps, verbose=1)\n",
        "valid_score = model.evaluate(valid_gen, steps=test_steps, verbose=1)\n",
        "test_score = model.evaluate(test_gen, steps=test_steps, verbose=1)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train Accuracy: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation Accuracy: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test Accuracy: \", test_score[1])\n",
        "\n",
        "# Generate predictions using the test generator\n",
        "preds = model.predict_generator(test_gen)\n",
        "y_pred = np.argmax(preds, axis=1)  # Get the predicted class indices\n",
        "print(y_pred)\n",
        "\n",
        "# Get class indices from the test generator\n",
        "g_dict = test_gen.class_indices\n",
        "classes = list(g_dict.keys())\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(test_gen.classes, y_pred, target_names=classes))\n",
        "\n",
        "# Define model and save parameters\n",
        "model_name = model.input_names[0][:-6]  # Extract model name by trimming the input layer suffix\n",
        "subject = 'Plant Village Disease'\n",
        "acc = test_score[1] * 100  # Convert accuracy to percentage\n",
        "save_path = ''  # Define save path (empty for current directory)\n",
        "\n",
        "# Save the trained model\n",
        "save_id = str(f'{model_name}-{subject}-{\"%.2f\" % round(acc, 2)}.h5')\n",
        "model_save_loc = os.path.join(save_path, save_id)\n",
        "model.save(model_save_loc)\n",
        "print(f'model was saved as {model_save_loc}')\n",
        "\n",
        "# Save the model weights separately\n",
        "weight_save_id = str(f'{model_name}-{subject}-weights.h5')\n",
        "weights_save_loc = os.path.join(save_path, weight_save_id)\n",
        "model.save_weights(weights_save_loc)\n",
        "print(f'weights were saved as {weights_save_loc}')\n",
        "\n",
        "# Generate class dictionary DataFrame\n",
        "class_dict = train_gen.class_indices\n",
        "img_size = train_gen.image_shape\n",
        "height = [img_size[0]] * len(class_dict)  # List of image heights\n",
        "width = [img_size[1]] * len(class_dict)  # List of image widths\n",
        "\n",
        "# Create Series for DataFrame\n",
        "Index_series = pd.Series(list(class_dict.values()), name='class_index')\n",
        "Class_series = pd.Series(list(class_dict.keys()), name='class')\n",
        "Height_series = pd.Series(height, name='height')\n",
        "Width_series = pd.Series(width, name='width')\n",
        "\n",
        "# Concatenate Series into DataFrame\n",
        "class_df = pd.concat([Index_series, Class_series, Height_series, Width_series], axis=1)\n",
        "\n",
        "# Save class dictionary DataFrame to CSV\n",
        "csv_name = f'{subject}-class_dict.csv'\n",
        "csv_save_loc = os.path.join(save_path, csv_name)\n",
        "class_df.to_csv(csv_save_loc, index=False)\n",
        "print(f'class csv file was saved as {csv_save_loc}')\n"
      ],
      "metadata": {
        "id": "hwiOG6Qvod_U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}